










## <$0.02

cost to generate 1000 lines of code  
(gpt‑4.1‑mini, apr 2025)














## whoami / tomoro.ai

- tom mclaughlin – applied ai solutions architect @ tomoro.ai  
- lean 2–3‑engineer squads, mvp → prod in < 12 wks  
- clients: red bull • nba • supercell • + more













## the (hello) world has changed

- chatgpt (and friends) landed nov '22  
- cost of writing that first version / mvp → approaching zero  
- ‘anyone’ can build now (sort of)













## nov 2022: chatgpt lands

- who remembers their first time using it? getting code back in seconds?  
- personal usage: daily since launch – docs, code, design reviews, architecture, *learning*  
- it accelerates every step of the feedback loop  
- it is also `training data` itself (good **and** bad)  
- time to adapt to *this* new environment and wield the new tools  













## so, what skills matter most?

- if that first build of your product is (almost) free, where’s the leverage?  
- **adaptability** – your personal rate of learning  
- that’s it: how quickly you can fit to a new problem, tool, or environment













## the learning‑rate loop  (borrowed from ml)

if adaptability == learning‑rate, how do we crank it up?

┌─ 1. evaluation metric ─ how do we define “good”? ────────────────┐  
│                                                                  ↓  
│  2. training data ─ deliberate experiences you feed yourself     │  
│                                                                  ↓  
│  3. fit & test ─ try → get feedback → adjust                     │  
└──────────────────────────────────────────────────────────────────┘  
↻ repeat faster  ➜  **higher adaptability**  (that’s the winning skill)

*(let’s watch that loop in my own career — three snapshots…)*













## adapting through experience: my training data













## example 1: school → uni

- **environment:** high school → cs @ strathclyde, 2nd yr  
- **evaluation metric:** passing exams (same metric)  
- **initial approach:** coasted on school habits → bombed uni exams (`poor generalisation`)  
- **adaptation:** fresh `training data` – uni past papers, lecturer feedback  
- **outcome:** swift recovery → easy A’s (`good fit` again)













## example 2: corporate swe (jp morgan)

- **environment:** large investment bank, java dev  
- **evaluation metric:** promotions, politics, code complexity (**metric shifted**)  
- **initial approach:** leaned on academic success → mis‑alignment (`poor fit`)  
- **adaptation:** learnt enterprise tooling, promotion paths, office politics  
- **outcome:** aligned to new `eval metrics`; career traction resumed













## example 3: startups (crypto, defence, consulting)

- **environment:** fast‑moving startups & boutique consultancies  
- **evaluation metric:** execution speed **plus** clear business impact & domain passion  
- **initial approach:** shipped quickly but passion/ROI alignment varied (`mixed fit`)  
- **adaptation:** shortest `fitting loops` — tight feedback, quick pivots, value focus  
- **outcome:** refined personal metrics — *love the domain, prove ROI fast*; learning rate quickest yet  

*(chatgpt now makes every loop 10× faster — here’s why that matters…)*













## learning loops: easier now… or harder?

- early llm coding: code → error → paste error → fix → repeat — painful but educational  
- today many simple tasks are one‑shot correct — convenient, but fewer failure signals  
- risk: surface‑level competence without depth  
- question: how do we ensure deep learning when the tool is *too* good?













## the strategy: seek complexity (& failure)

- if simple tasks lack learning friction… push higher  
- **give llms more ambitious goals** – stretch their limits (and yours)  
- expect failure; welcome it  
- **reframe failure:** rich, high‑quality `training data` — where the *real* learning starts













## learning from complexity: the process

1. **break it down** — decompose the failure into smaller sub‑problems  
2. **diagnose together** — use your intuition *and* the llm to ask *why*  
3. **iterate on fixes** — test → refine → repeat, side‑by‑side with the model  
4. **synthesise** — solve enough small problems, the big one falls; loop speeds up













## the payoff: accelerating your learning rate

- seek complexity → collaborative failure analysis = learning‑rate engine  
- you’re upgrading skills **while** solving the problem  
- faster loop = greater adaptability = higher leverage in the post‑llm world













## applying the loop: beyond code

- the framework works for any knowledge work: research, design, ops, strategy  
- adopt it in your day job: deliberately seek gnarly tasks, run the loop, reflect  
- build the habit; the loop compounds













## maximum leverage: high‑intensity environments

- want to *turbo‑charge* the loop? raise the temperature  
- **start‑ups:** real problems under pressure — ultimate fit test  
- **hackathons:** diverse problems fast; dense feedback  
- **accelerators (yc, ef, etc.):** pressure‑cookers forcing rapid adaptation  
- these spaces supply dense, high‑quality `training data`













## tl;dr / key takeaways

- **adaptability (learning‑rate)** is the scarce skill now  
- think like ml: set `eval metrics`, gather `training data`, iterate your `fit`  
- use llms to accelerate, but seek complexity to maintain depth  
- apply everywhere — day job, start‑ups, hackathons, accelerators  
- **tonight:** pick one task you’d usually avoid, push an llm until it breaks, learn from the failure













## thanks / questions?

- linkedin: tom mclaughlin  •  /in/aibuilder











