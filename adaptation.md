










## < $ 0.02

cost to generate 1000 lines of code  
(gpt‑4.1‑mini, apr 2025)















## whoami
- tom mclaughlin
- applied ai solutions architect
- tomoro.ai













## tomoro ai

### what
- applied ai consultancy
- build cutting-edge generative ai solutions for enterprise clients [insert rolodex of household names]
- scale to [insert impressive metric here]

### how
- working as lean engineering teams (2/3 people)
- deeply embedded into our client problems and processes
- build mvp to prod in < 12 weeks
- verify
- scale













## the (hello) world has changed

- chatgpt (and friends) landed nov '22
- cost of writing that first version / mvp -> approaching zero
- 'anyone' can build now (sort of)














## so, what skills matter most?

- if that first build of your app/product is (almost) free, where's the leverage?

- **adaptability** (aka rate of learning)

- that's it. how quickly you can fit to a new problem, tool, or environment.














## improving adaptability: learning like machines?

- if adaptability == rate of learning... how do we speed it up?
- maybe we can borrow ideas from how machines learn.
- think of your career progression as 'training' yourself.














## the ml framework for learning

- **1. evaluation metrics:**
    - how do you measure 'good' in a role/environment? (your 'fitness function')
- **2. training data:**
    - what experiences (good & bad) are you learning from? (quality matters: garbage in, garbage out)
- **3. fitting:**
    - using data to meet the metrics. (underfitting vs overfitting vs good fit/generalization)
- **the loop:** define metrics -> gather data -> evaluate fit -> adjust -> repeat














## adapting through experience: my training data

- let's look at some past 'environments' through this ml lens...














## example 1: school exams

- **environment:** scottish high school exams
- **evaluation metric:** passing grades (realisation: metric heavily weighted on past paper performance)
- **initial approach:** lazy kid, loved games, didn't focus on metric
- **result:** bombed exams (`underfitting`)
- **adaptation:** changed `training data` -> focused heavily on past papers
- **outcome:** 6 'a' grades (`good fit` achieved for that specific metric)














## example 2: university exams

- **environment:** university (cs @ strathclyde), 2nd year
- **evaluation metric:** passing exams (again, past papers were key)
- **initial approach:** coasting off school success (poor `generalization` from previous environment?)
- **result:** bombed exams (`poor fit`)
- **adaptation:** sought feedback (lecturers), updated `training data` -> focused on *uni* past papers
- **outcome:** found it easy (`good fit` achieved again)














## example 3: corporate swe (jp morgan)

- **environment:** large investment bank, corporate java dev
- **evaluation metric:** changed significantly! (corporate standards, promotions, politics != academic grades)
- **initial approach:** relying on degree success (failed to `generalize` again)
- **result:** 'goal posts moved', uni java skills != corporate java needs (`poor fit` initially)
- **adaptation:** required learning new technical standards, understanding promotion paths, navigating politics ('play the game')
- **outcome:** needed significant adaptation to new `eval metrics` & environment.














## example 4: crypto startup

- **environment:** fast-paced startup, crypto domain
- **evaluation metric:** speed of execution ('build fast: now or sooner'), domain obsession/passion.
- **initial approach:** technically decent, u-turn on corporate process (conscious adaptation attempt).
- **result:** built fast, but realised lack of passion (`poor fit` on 'passion' metric).
- **adaptation:** pivoted after 7 months (faster `fitting loop` / `verification` than corporate).
- **outcome:** learned importance of domain passion as a personal `eval metric`. faster adaptation cycle.














## example 5: defence consultancy

- **environment:** boutique consultancy, gov sector, high-quality engineering focus
- **evaluation metric:** engineering rigour, quality, risk mitigation, cultural fit.
- **initial approach:** brought experiences from corporate/startup.
- **result:** strong alignment with mission/culture (`good fit`). exposed to excellent engineers (high-quality `training data`).
- **adaptation:** adjusted to extremely high-quality standards & low-risk profile (contrast to startup speed).
- **outcome:** successful `fit`, gained valuable high-quality engineering `training data`. (environment changed later due to acquisition).














## example 6: contract consulting

- **environment:** independent consulting, client business focus
- **evaluation metric:** defining scope, cost, time, roi; building & selling a business case.
- **initial approach:** leveraging technical background.
- **result:** needed to rapidly acquire business acumen (potential `poor fit` on business metrics initially).
- **adaptation:** focused learning on business needs, communication, value proposition (`fit to business`).
- **outcome:** developed skills translating technical solutions to business value.














## the accelerator: enter chatgpt (& friends)

- november 2022 changed the game *again*.
- who remembers their first time using it? getting code from it?
- personal usage: daily since launch (docs, code, review, architecture, *learning*, cv tailoring, checking this talk...).
- it massively speeds up parts of the `fitting loop`.
- it's also a source of `training data` itself (good and bad!).
- resistance is futile? ("but we shouldn't use it for docs!" - why not?)
- **story time:** [insert brief, impactful story of using chatgpt for learning/adaptation]
- time to adapt to *this* new environment... and use the new tools.














## learning loops: easier now... or harder?

- remember early llm coding? code -> error -> paste error -> fix -> repeat.
- that painful loop was valuable: failures were frequent `training data` points. you *had* to learn why it broke.
- current llms often 'one-shot' simple tasks correctly. convenient, but...
- ...do we lose those 'free' learning opportunities when things *don't* break?
- how do we ensure deep learning when the tool gets *too* good?














## the strategy: seek complexity (& failure)

- if simple tasks lack learning friction... aim higher.
- **give llms more ambitious goals.** push their limits (and yours).
- tackle problems where failure *is* likely, even expected.
- **reframe failure:** it's not the end, it's high-quality `training data`. it's where the *real* learning starts.














## learning from complexity: the process

- failure signals the start of collaborative analysis.
- **1. break it down:** decompose the complex failure into smaller sub-problems.
- **2. diagnose together:** use your intuition *and* the llm to understand *why* each part failed.
- **3. iterate on fixes:** work *with* the llm to solve the sub-problems. test -> refine -> repeat.
- **4. synthesize:** solving enough small problems solves the big one. this *is* the accelerated learning loop.














## the payoff: accelerating your learning rate

- this process (seek complexity -> collaborative failure analysis) *is* the engine.
- you're not just solving the problem; you're actively upgrading your skills & understanding faster.
- this directly increases your **adaptability / rate of learning**.
- this is how you build leverage and thrive in the post-llm world.














## applying the loop: beyond code

- this framework isn't just for software engineers writing code.
- seek complexity -> learn from failure -> accelerate learning... applies to *any* knowledge work.
- research, design, analysis, strategy, writing... you name it.
- start by applying it consistently in your day job. build the habit.














## maximum leverage: high-intensity environments

- want to *turbo-charge* your learning rate? seek high-intensity exposure.
- **1. startups:** join one or start one. solve real problems under pressure. ultimate `fit` test.
- **2. hackathons:** tackle diverse problems fast. collaborate with motivated peers. intense `fitting loops`.
- **3. accelerators (yc, ef, etc.):** condensed startup/hackathon pressure cooker. forces rapid adaptation.
- these environments provide dense, high-quality `training data` and demand accelerated learning.














## tl;dr / key takeaways

- **adaptability (rate of learning)** is the crucial skill now.
- think like ml: define `eval metrics`, get `training data` (exposure!), iterate your `fit`.
- use llms to accelerate, but seek complexity to ensure deep learning from (collaborative) failure analysis.
- apply everywhere: day job, startups, hackathons, accelerators. maximize exposure.
















## thanks / questions?

- find me on linkedin: 
  - tom mclaughlin 
  - /in/aibuilder








